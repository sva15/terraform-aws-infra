# 🔄 Hybrid Configuration Setup - Local + S3 Sources

## ✅ **Configuration Updated for Hybrid Approach**

Your `terraform.tfvars` has been configured for a **hybrid approach**:
- **Lambda Code**: Local path (uploaded by Terraform)
- **Lambda Layers**: S3 path (existing files in S3)
- **Database SQL**: S3 path (existing files in S3)
- **db-restore**: Auto-generated function (no separate zip)

## 🔧 **Updated Configuration**

### **✅ Hybrid terraform.tfvars:**
```hcl
# Lambda Configuration (HYBRID: Local code + S3 layers)
lambda_prefix            = "dev-ifrs"
use_local_source         = true                                    # Upload Lambda code from local
artifacts_s3_bucket      = "filterrithas"                         # Existing S3 bucket
create_s3_bucket         = false                                   # Don't create new bucket
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"  # Local Lambda code
lambda_layers_local_path = ""                                     # Empty = use S3 layers, not local
ui_assets_local_path     = "../../ui"

# Alternative local paths (commented for future use)
# lambda_code_local_path   = "C:/path/to/your/lambda/functions"
# lambda_layers_local_path = "C:/path/to/your/lambda/layers"
# ui_assets_local_path     = "C:/path/to/your/ui/build"

# Lambda Layer Mappings (layers from S3, not local)
lambda_layer_mappings = {
  "sns-lambda" = ["lambda-deps-layer"]  # Use existing layer from S3
  "alb-lambda" = ["alb-layer"]          # Use existing layer from S3
  # db-restore function is auto-generated, no separate layer needed
}

# SQL Backup Configuration (S3 path - existing files)
sql_backup_s3_bucket  = "filterrithas"                    # Existing S3 bucket
sql_backup_s3_key     = "postgres/ifrs_backup_20250928_144411.sql"  # Existing SQL file in S3
sql_backup_local_path = ""                                 # Empty = use S3 files, not local

# Alternative local SQL backup path (commented for future use)
# sql_backup_local_path = "../../database/pg_backup"
# sql_backup_local_path = "C:/path/to/your/sql/backups"
```

## 📊 **How Hybrid Configuration Works**

### **🔄 Source Logic:**
```
Component               Source      Path/Location
├── Lambda Functions   → Local   → ../../backend/python-aws-lambda-functions/
├── Lambda Layers      → S3      → s3://filterrithas/layers/
├── Database SQL       → S3      → s3://filterrithas/postgres/
├── UI Assets          → Local   → ../../ui/
└── db-restore         → Generated by RDS module (no zip file)
```

### **📁 Expected File Structure:**

#### **✅ Local Files (will be uploaded):**
```
backend/python-aws-lambda-functions/
├── sns-lambda.zip          # ← Local file, uploaded to S3
├── alb-lambda.zip          # ← Local file, uploaded to S3
└── (no db-restore.zip)     # ← Generated function, not a file

ui/
└── ui-assets.zip           # ← Local file, uploaded to S3
```

#### **✅ S3 Files (existing, referenced):**
```
s3://filterrithas/
├── layers/
│   ├── lambda-deps-layer.zip   # ← Existing in S3, referenced by sns-lambda
│   └── alb-layer.zip           # ← Existing in S3, referenced by alb-lambda
└── postgres/
    └── ifrs_backup_20250928_144411.sql  # ← Existing in S3, used by db-restore
```

## 🚀 **Deployment Flow**

### **1. ✅ Local → S3 Upload:**
```bash
# Terraform will upload these local files:
Local Files                                    →  S3 Destination
├── backend/python-aws-lambda-functions/
│   ├── sns-lambda.zip                        →  s3://filterrithas/lambdas/sns-lambda.zip
│   └── alb-lambda.zip                        →  s3://filterrithas/lambdas/alb-lambda.zip
└── ui/ui-assets.zip                          →  s3://filterrithas/ui-build/ui-assets.zip
```

### **2. ✅ S3 → Lambda Layer References:**
```bash
# Lambda functions will reference existing S3 layers:
Lambda Function    →  Layer Reference
├── sns-lambda     →  s3://filterrithas/layers/lambda-deps-layer.zip
└── alb-lambda     →  s3://filterrithas/layers/alb-layer.zip
```

### **3. ✅ Auto-Generated Functions:**
```bash
# RDS module will generate db-restore function:
db-restore Lambda  →  Generated from template + S3 SQL file reference
└── References     →  s3://filterrithas/postgres/ifrs_backup_20250928_144411.sql
```

## 🔧 **Module Logic Updates**

### **✅ Lambda Module Enhanced:**
```hcl
# Now supports hybrid layer sources:
locals {
  # Local layer names (from local files)
  local_layer_names = [for file in lambda_layer_files : trimsuffix(file, ".zip")]
  
  # Mapped layer names (from terraform.tfvars mappings)
  mapped_layer_names = distinct(flatten([for func, layers in var.lambda_layer_mappings : layers]))
  
  # Combined layer names (local + mapped)
  lambda_layer_names = distinct(concat(local.local_layer_names, local.mapped_layer_names))
}

# Layer creation logic:
resource "aws_lambda_layer_version" "layers" {
  # Creates layers for BOTH local files AND S3 references
  for_each = toset(local.lambda_layer_names)
  
  # Use local file if available, otherwise reference S3
  filename = contains(local.local_layer_names, each.value) ? "local_path" : null
  s3_bucket = "filterrithas"
  s3_key = "layers/${each.value}.zip"
}
```

## 📋 **Configuration Scenarios**

### **✅ Current Setup (Hybrid):**
```hcl
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"  # Local
lambda_layers_local_path = ""                                           # S3
sql_backup_local_path    = ""                                           # S3
```

### **🔄 Alternative: All Local (uncomment when needed):**
```hcl
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"
lambda_layers_local_path = "../../backend/lambda-layers"
sql_backup_local_path    = "../../database/pg_backup"
```

### **🔄 Alternative: All S3 (set paths to empty):**
```hcl
lambda_code_local_path   = ""  # Use S3
lambda_layers_local_path = ""  # Use S3
sql_backup_local_path    = ""  # Use S3
```

### **🔄 Alternative: Custom Paths (uncomment and modify):**
```hcl
lambda_code_local_path   = "C:/path/to/your/lambda/functions"
lambda_layers_local_path = "C:/path/to/your/lambda/layers"
sql_backup_local_path    = "C:/path/to/your/sql/backups"
```

## ⚠️ **Important Notes**

### **📋 File Requirements:**
```bash
# Required LOCAL files (must exist):
backend/python-aws-lambda-functions/
├── sns-lambda.zip     # ← Must exist locally
└── alb-lambda.zip     # ← Must exist locally

ui/
└── ui-assets.zip      # ← Must exist locally

# Required S3 files (must exist in S3):
s3://filterrithas/layers/
├── lambda-deps-layer.zip   # ← Must exist in S3
└── alb-layer.zip           # ← Must exist in S3

s3://filterrithas/postgres/
└── ifrs_backup_20250928_144411.sql  # ← Must exist in S3
```

### **🔧 Layer Naming:**
- **S3 Layer Names**: Must match exactly what's in your S3 bucket
- **Local Function Names**: Must match zip file names (without .zip)
- **Mapping**: `lambda_layer_mappings` connects functions to layers

### **🚀 db-restore Function:**
- **No separate zip file** needed
- **Auto-generated** by RDS module
- **References SQL file** from S3 directly
- **Includes restore logic** in the generated function

## 🎯 **Benefits of Hybrid Approach**

### **✅ Flexibility:**
- **Mix sources** as needed (local + S3)
- **Easy switching** between local and S3
- **Commented alternatives** for quick changes

### **✅ Efficiency:**
- **Reuse existing** S3 layers
- **Upload only** what changes (Lambda code)
- **No duplicate** layer uploads

### **✅ Development Workflow:**
- **Develop locally** (Lambda functions)
- **Stable layers** stay in S3
- **Quick iterations** on function code

## 🚀 **Ready to Deploy!**

Your hybrid configuration is set up to:

### ✅ **Upload from Local:**
- Lambda function code
- UI assets

### ✅ **Reference from S3:**
- Lambda layers (existing)
- SQL backup files (existing)

### ✅ **Auto-Generate:**
- db-restore Lambda function

**🔧 Run `terraform apply` to deploy with the hybrid configuration!**
