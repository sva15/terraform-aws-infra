# ğŸ”„ Hybrid Configuration Setup - Local + S3 Sources

## âœ… **Configuration Updated for Hybrid Approach**

Your `terraform.tfvars` has been configured for a **hybrid approach**:
- **Lambda Code**: Local path (uploaded by Terraform)
- **Lambda Layers**: S3 path (existing files in S3)
- **Database SQL**: S3 path (existing files in S3)
- **db-restore**: Auto-generated function (no separate zip)

## ğŸ”§ **Updated Configuration**

### **âœ… Hybrid terraform.tfvars:**
```hcl
# Lambda Configuration (HYBRID: Local code + S3 layers)
lambda_prefix            = "dev-ifrs"
use_local_source         = true                                    # Upload Lambda code from local
artifacts_s3_bucket      = "filterrithas"                         # Existing S3 bucket
create_s3_bucket         = false                                   # Don't create new bucket
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"  # Local Lambda code
lambda_layers_local_path = ""                                     # Empty = use S3 layers, not local
ui_assets_local_path     = "../../ui"

# Alternative local paths (commented for future use)
# lambda_code_local_path   = "C:/path/to/your/lambda/functions"
# lambda_layers_local_path = "C:/path/to/your/lambda/layers"
# ui_assets_local_path     = "C:/path/to/your/ui/build"

# Lambda Layer Mappings (layers from S3, not local)
lambda_layer_mappings = {
  "sns-lambda" = ["lambda-deps-layer"]  # Use existing layer from S3
  "alb-lambda" = ["alb-layer"]          # Use existing layer from S3
  # db-restore function is auto-generated, no separate layer needed
}

# SQL Backup Configuration (S3 path - existing files)
sql_backup_s3_bucket  = "filterrithas"                    # Existing S3 bucket
sql_backup_s3_key     = "postgres/ifrs_backup_20250928_144411.sql"  # Existing SQL file in S3
sql_backup_local_path = ""                                 # Empty = use S3 files, not local

# Alternative local SQL backup path (commented for future use)
# sql_backup_local_path = "../../database/pg_backup"
# sql_backup_local_path = "C:/path/to/your/sql/backups"
```

## ğŸ“Š **How Hybrid Configuration Works**

### **ğŸ”„ Source Logic:**
```
Component               Source      Path/Location
â”œâ”€â”€ Lambda Functions   â†’ Local   â†’ ../../backend/python-aws-lambda-functions/
â”œâ”€â”€ Lambda Layers      â†’ S3      â†’ s3://filterrithas/layers/
â”œâ”€â”€ Database SQL       â†’ S3      â†’ s3://filterrithas/postgres/
â”œâ”€â”€ UI Assets          â†’ Local   â†’ ../../ui/
â””â”€â”€ db-restore         â†’ Generated by RDS module (no zip file)
```

### **ğŸ“ Expected File Structure:**

#### **âœ… Local Files (will be uploaded):**
```
backend/python-aws-lambda-functions/
â”œâ”€â”€ sns-lambda.zip          # â† Local file, uploaded to S3
â”œâ”€â”€ alb-lambda.zip          # â† Local file, uploaded to S3
â””â”€â”€ (no db-restore.zip)     # â† Generated function, not a file

ui/
â””â”€â”€ ui-assets.zip           # â† Local file, uploaded to S3
```

#### **âœ… S3 Files (existing, referenced):**
```
s3://filterrithas/
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ lambda-deps-layer.zip   # â† Existing in S3, referenced by sns-lambda
â”‚   â””â”€â”€ alb-layer.zip           # â† Existing in S3, referenced by alb-lambda
â””â”€â”€ postgres/
    â””â”€â”€ ifrs_backup_20250928_144411.sql  # â† Existing in S3, used by db-restore
```

## ğŸš€ **Deployment Flow**

### **1. âœ… Local â†’ S3 Upload:**
```bash
# Terraform will upload these local files:
Local Files                                    â†’  S3 Destination
â”œâ”€â”€ backend/python-aws-lambda-functions/
â”‚   â”œâ”€â”€ sns-lambda.zip                        â†’  s3://filterrithas/lambdas/sns-lambda.zip
â”‚   â””â”€â”€ alb-lambda.zip                        â†’  s3://filterrithas/lambdas/alb-lambda.zip
â””â”€â”€ ui/ui-assets.zip                          â†’  s3://filterrithas/ui-build/ui-assets.zip
```

### **2. âœ… S3 â†’ Lambda Layer References:**
```bash
# Lambda functions will reference existing S3 layers:
Lambda Function    â†’  Layer Reference
â”œâ”€â”€ sns-lambda     â†’  s3://filterrithas/layers/lambda-deps-layer.zip
â””â”€â”€ alb-lambda     â†’  s3://filterrithas/layers/alb-layer.zip
```

### **3. âœ… Auto-Generated Functions:**
```bash
# RDS module will generate db-restore function:
db-restore Lambda  â†’  Generated from template + S3 SQL file reference
â””â”€â”€ References     â†’  s3://filterrithas/postgres/ifrs_backup_20250928_144411.sql
```

## ğŸ”§ **Module Logic Updates**

### **âœ… Lambda Module Enhanced:**
```hcl
# Now supports hybrid layer sources:
locals {
  # Local layer names (from local files)
  local_layer_names = [for file in lambda_layer_files : trimsuffix(file, ".zip")]
  
  # Mapped layer names (from terraform.tfvars mappings)
  mapped_layer_names = distinct(flatten([for func, layers in var.lambda_layer_mappings : layers]))
  
  # Combined layer names (local + mapped)
  lambda_layer_names = distinct(concat(local.local_layer_names, local.mapped_layer_names))
}

# Layer creation logic:
resource "aws_lambda_layer_version" "layers" {
  # Creates layers for BOTH local files AND S3 references
  for_each = toset(local.lambda_layer_names)
  
  # Use local file if available, otherwise reference S3
  filename = contains(local.local_layer_names, each.value) ? "local_path" : null
  s3_bucket = "filterrithas"
  s3_key = "layers/${each.value}.zip"
}
```

## ğŸ“‹ **Configuration Scenarios**

### **âœ… Current Setup (Hybrid):**
```hcl
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"  # Local
lambda_layers_local_path = ""                                           # S3
sql_backup_local_path    = ""                                           # S3
```

### **ğŸ”„ Alternative: All Local (uncomment when needed):**
```hcl
lambda_code_local_path   = "../../backend/python-aws-lambda-functions"
lambda_layers_local_path = "../../backend/lambda-layers"
sql_backup_local_path    = "../../database/pg_backup"
```

### **ğŸ”„ Alternative: All S3 (set paths to empty):**
```hcl
lambda_code_local_path   = ""  # Use S3
lambda_layers_local_path = ""  # Use S3
sql_backup_local_path    = ""  # Use S3
```

### **ğŸ”„ Alternative: Custom Paths (uncomment and modify):**
```hcl
lambda_code_local_path   = "C:/path/to/your/lambda/functions"
lambda_layers_local_path = "C:/path/to/your/lambda/layers"
sql_backup_local_path    = "C:/path/to/your/sql/backups"
```

## âš ï¸ **Important Notes**

### **ğŸ“‹ File Requirements:**
```bash
# Required LOCAL files (must exist):
backend/python-aws-lambda-functions/
â”œâ”€â”€ sns-lambda.zip     # â† Must exist locally
â””â”€â”€ alb-lambda.zip     # â† Must exist locally

ui/
â””â”€â”€ ui-assets.zip      # â† Must exist locally

# Required S3 files (must exist in S3):
s3://filterrithas/layers/
â”œâ”€â”€ lambda-deps-layer.zip   # â† Must exist in S3
â””â”€â”€ alb-layer.zip           # â† Must exist in S3

s3://filterrithas/postgres/
â””â”€â”€ ifrs_backup_20250928_144411.sql  # â† Must exist in S3
```

### **ğŸ”§ Layer Naming:**
- **S3 Layer Names**: Must match exactly what's in your S3 bucket
- **Local Function Names**: Must match zip file names (without .zip)
- **Mapping**: `lambda_layer_mappings` connects functions to layers

### **ğŸš€ db-restore Function:**
- **No separate zip file** needed
- **Auto-generated** by RDS module
- **References SQL file** from S3 directly
- **Includes restore logic** in the generated function

## ğŸ¯ **Benefits of Hybrid Approach**

### **âœ… Flexibility:**
- **Mix sources** as needed (local + S3)
- **Easy switching** between local and S3
- **Commented alternatives** for quick changes

### **âœ… Efficiency:**
- **Reuse existing** S3 layers
- **Upload only** what changes (Lambda code)
- **No duplicate** layer uploads

### **âœ… Development Workflow:**
- **Develop locally** (Lambda functions)
- **Stable layers** stay in S3
- **Quick iterations** on function code

## ğŸš€ **Ready to Deploy!**

Your hybrid configuration is set up to:

### âœ… **Upload from Local:**
- Lambda function code
- UI assets

### âœ… **Reference from S3:**
- Lambda layers (existing)
- SQL backup files (existing)

### âœ… **Auto-Generate:**
- db-restore Lambda function

**ğŸ”§ Run `terraform apply` to deploy with the hybrid configuration!**
